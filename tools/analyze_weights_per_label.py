"""
Script ph√¢n t√≠ch tr·ªçng s·ªë body part theo t·ª´ng label (action class).
Ki·ªÉm tra xem trong c√πng 1 action, c√°c sample c√≥ weight gi·ªëng nhau kh√¥ng.
N·∫øu weight qu√° kh√°c nhau (std cao) ‚Üí CTR-GCN ƒëang cho importance kh√¥ng nh·∫•t qu√°n
‚Üí weight s·∫Ω g√¢y nhi·ªÖu cho ResNet thay v√¨ gi√∫p √≠ch.
"""
import torch
import numpy as np
import sys
import os
from tqdm import tqdm
from collections import defaultdict

sys.path.append(os.getcwd())
from models.ctrgcn import Model as CTRGCN
from feeder.feeder_nucla_fusion import Feeder

output_device = 0 if torch.cuda.is_available() else 'cpu'
device = torch.device(f"cuda:{output_device}" if torch.cuda.is_available() else "cpu")

NUM_BODY_PARTS = 5
TARGET_JOINTS = [3, 11, 7, 18, 14]
PARTS_NAMES = ['head', 'l_hand', 'r_hand', 'l_leg', 'r_leg']

# UCLA action labels (0-indexed, label trong data_dict l√† 1-indexed)
ACTION_NAMES = {
    1: 'pick up with one hand',
    2: 'pick up with two hands', 
    3: 'drop trash',
    4: 'walk around',
    5: 'sit down',
    6: 'stand up',
    7: 'donning',
    8: 'doffing',
    9: 'throw',
    10: 'carry',
}


def analyze_weights(weights_path):
    print(">>> ƒêang kh·ªüi t·∫°o m√¥ h√¨nh CTR-GCN...")
    graph_args = {'labeling_mode': 'spatial'}
    model_ske = CTRGCN(
        num_class=10, 
        num_point=20, 
        num_person=1,
        graph='graph.ucla.Graph',
        graph_args=graph_args,
        in_channels=3
    ).to(device)

    if weights_path and os.path.exists(weights_path):
        print(f">>> Loading weights t·ª´: {weights_path}")
        try:
            model_ske.load_state_dict(torch.load(weights_path))
            print(">>> Load weights th√†nh c√¥ng!")
        except Exception as e:
            print(f"!!! Kh√¥ng load ƒë∆∞·ª£c weights ({e}). D√πng weights ng·∫´u nhi√™n.")
    else:
        print("!!! Kh√¥ng t√¨m th·∫•y weights. D√πng weights ng·∫´u nhi√™n ‚Üí k·∫øt qu·∫£ s·∫Ω v√¥ nghƒ©a!")

    model_ske.eval()

    # Thu th·∫≠p weights theo label
    # Key: label, Value: list of weight arrays (m·ªói array c√≥ 5 ph·∫ßn t·ª≠)
    weights_by_label = defaultdict(list)
    
    for split in ['train', 'val']:
        print(f"\n>>> ƒêang x·ª≠ l√Ω: {split}")
        feeder = Feeder(
            split=split,
            random_choose=False,
            random_shift=False,
            random_move=False,
            window_size=50,
            temporal_rgb_frames=5
        )
        
        loader = torch.utils.data.DataLoader(
            dataset=feeder, batch_size=1, shuffle=False, num_workers=0
        )

        for i, (data, label, index) in enumerate(tqdm(loader)):
            data_ske = data[0].float().to(device)
            lbl = label.item()
            
            N, C, T, V, M = data_ske.size()
            
            with torch.no_grad():
                _, feature = model_ske.extract_feature(data_ske)
                
            intensity_s = (feature * feature).sum(dim=1) ** 0.5
            intensity_s = intensity_s.cpu().detach().numpy()
            feature_s = np.abs(intensity_s)
            
            feat_min, feat_max = feature_s.min(), feature_s.max()
            if (feat_max - feat_min) > 0:
                feature_s = (feature_s - feat_min) / (feat_max - feat_min)
            
            weights_per_part = np.ones(NUM_BODY_PARTS)
            n = 0
            person_idx = 0
            
            _, _, V_feat, M_feat = feature_s.shape
            
            temporal_positions = 15
            for j, v_idx in enumerate(TARGET_JOINTS):
                if v_idx < V_feat:
                    feature_val = feature_s[n, :, v_idx, person_idx]
                    k = min(temporal_positions, len(feature_val))
                    top_k_vals = np.partition(feature_val, -k)[-k:]
                    weights_per_part[j] = top_k_vals.mean()
            
            # Normalize v·ªÅ [0.5, 1.5] nh∆∞ script ch√≠nh
            w_min, w_max = weights_per_part.min(), weights_per_part.max()
            if (w_max - w_min) > 0:
                weights_per_part = 0.5 + 1.0 * (weights_per_part - w_min) / (w_max - w_min)
            else:
                weights_per_part = np.ones(NUM_BODY_PARTS)
            
            weights_by_label[lbl].append(weights_per_part)

    # ===== IN K·∫æT QU·∫¢ =====
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH TR·ªåNG S·ªê BODY PART THEO T·ª™NG LABEL")
    print("=" * 80)
    print(f"{'':>5} | {'head':>12} | {'l_hand':>12} | {'r_hand':>12} | {'l_leg':>12} | {'r_leg':>12} | {'Consistency':>12}")
    print("-" * 95)
    
    for lbl in sorted(weights_by_label.keys()):
        ws = np.array(weights_by_label[lbl])  # (num_samples, 5)
        mean_w = ws.mean(axis=0)
        std_w = ws.std(axis=0)
        avg_std = std_w.mean()
        
        action = ACTION_NAMES.get(lbl, f'action_{lbl}')
        
        # In trung b√¨nh
        w_str = " | ".join([f"{mean_w[j]:.3f}¬±{std_w[j]:.2f}" for j in range(NUM_BODY_PARTS)])
        
        # ƒê√°nh gi√° consistency: std th·∫•p = nh·∫•t qu√°n, std cao = kh√¥ng nh·∫•t qu√°n
        if avg_std < 0.05:
            consistency = "‚úì T·ªët"
        elif avg_std < 0.12:
            consistency = "~ Trung b√¨nh"
        else:
            consistency = "‚úó K√©m"
        
        print(f"L{lbl:>2}  | {w_str} | {consistency:>12}")
        
    print("-" * 95)
    
    # In ph√¢n t√≠ch chi ti·∫øt
    print("\n" + "=" * 80)
    print("CHI TI·∫æT: Body part n√†o b·ªã L√ÄM M·ªú / TƒÇNG S√ÅNG theo t·ª´ng action")
    print("=" * 80)
    
    for lbl in sorted(weights_by_label.keys()):
        ws = np.array(weights_by_label[lbl])
        mean_w = ws.mean(axis=0)
        std_w = ws.std(axis=0)
        avg_std = std_w.mean()
        
        action = ACTION_NAMES.get(lbl, f'action_{lbl}')
        n_samples = len(ws)
        
        print(f"\n--- Label {lbl}: {action} ({n_samples} samples, avg_std={avg_std:.4f}) ---")
        
        # S·∫Øp x·∫øp body parts theo importance
        sorted_parts = np.argsort(mean_w)  # ascending ‚Üí ph·∫ßn t·ª≠ ƒë·∫ßu = √≠t quan tr·ªçng nh·∫•t
        
        darkened = []
        brightened = []
        neutral = []
        
        for idx in sorted_parts:
            w = mean_w[idx]
            s = std_w[idx]
            name = PARTS_NAMES[idx]
            if w < 0.8:
                darkened.append(f"{name} (w={w:.3f}¬±{s:.2f})")
            elif w > 1.2:
                brightened.append(f"{name} (w={w:.3f}¬±{s:.2f})")
            else:
                neutral.append(f"{name} (w={w:.3f}¬±{s:.2f})")
        
        if brightened:
            print(f"  üîÜ TƒÉng s√°ng: {', '.join(brightened)}")
        if neutral:
            print(f"  ‚ö™ Trung t√≠nh: {', '.join(neutral)}")
        if darkened:
            print(f"  üîÖ L√†m m·ªù:    {', '.join(darkened)}")
        
        if avg_std > 0.12:
            print(f"  ‚ö†Ô∏è  STD CAO ({avg_std:.3f}): Weight KH√îNG nh·∫•t qu√°n gi·ªØa c√°c sample ‚Üí g√¢y nhi·ªÖu!")


if __name__ == '__main__':
    WEIGHTS_PATH = './result/nucla/CTROGC-GCN.pt'
    analyze_weights(WEIGHTS_PATH)
